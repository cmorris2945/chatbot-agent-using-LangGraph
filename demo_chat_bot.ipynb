{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2304e98e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## This is  a chatbot that uses tools to answer questions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cf39c7e4",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'langchain_openai'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_openai\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ChatOpenAI\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_community\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtools\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TavilySearchResults\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_core\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtools\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tools\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'langchain_openai'"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_community.tools import TavilySearchResults\n",
    "from langchain_core.tools import tools\n",
    "from langgraph.graph import StateGraph\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "\n",
    "from datetime import datetime\n",
    "from typing import Annotated\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "_ = load_dotenv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98da6158",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model=\"gpt-4o\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94116904",
   "metadata": {},
   "outputs": [],
   "source": [
    "tavily_search = TavilySearchResults(max_results=2)\n",
    "\n",
    "@tool\n",
    "def get_current_date():\n",
    "    \"\"\"Returns the current date and time. Use this tool first for any 'time-based' questions.\"\"\"\n",
    "    return f\"The current date is: {datetime.now().strftime('%d %B %Y')}\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd86d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [tavily_search, get_current_date]\n",
    "llm_with_tools= llm.bind_tools(tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2963680",
   "metadata": {},
   "outputs": [],
   "source": [
    "class State(TypedDict):\n",
    "    messages: Annotated[list, add_messages]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54300e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create chatbot here....\n",
    "\n",
    "def chatbot(state: State):\n",
    "    return {\"messages\": [llm_with_tools.invoke(state[\"messages\"])]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6447080",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Have to have a graph first to use the chatbot node\n",
    "graph_builder = StateGraph(State)\n",
    "\n",
    "graph_builder.add_node(\"chatbot\", chatbot)\n",
    "\n",
    "tool_node = ToolNode(tools=tools)\n",
    "graph_builder.add_node(\"tools\",tool_node)\n",
    "\n",
    "\n",
    "### NOTE FOR MYSELF: This is a conditional edge Im making to do the actual decision whether to use the tools/functions or\n",
    "## not. This in particular is a prebuilt conditional edge...\n",
    "graph_builder.add_conditional_edges(\"chatbot\", tools_condition)\n",
    "\n",
    "\n",
    "## Now if the tools are not used,\n",
    "## then Im just going to return to the chatbot and process the tool output...\n",
    "graph_builder.add_edge(\"tools\",\"chatbot\")\n",
    "\n",
    "## NOTE: You have to set the entry point to the graph cycle...\n",
    "\n",
    "graph_builder.set_entry_point(\"chatbot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55aef78f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## I add human in the look checkpoint here...\n",
    "\n",
    "memory = MemorySaver()\n",
    "\n",
    "graph = graph_builder.compile(checkpointer=memory,\n",
    "                              interupt_before=[\"tools\"])  #<-- interupt before executing tools\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd8d385",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Image\n",
    "\n",
    "# Visualize the graph...\n",
    "\n",
    "try:\n",
    "    display(Image(graph.get_graph().draw_mermaid_png()))\n",
    "except Exception:\n",
    "    pass ## visualization needs more stuff...\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a7b7ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "## run graph here...\n",
    "\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "def render_markedown(md_string):\n",
    "    display(Markdown(md_string))\n",
    "\n",
    "def process_stream(stream):\n",
    "    for s in stream:\n",
    "        message = s[\"messages\"][-1]\n",
    "        if isinstance(message,tuple):\n",
    "            print(message)\n",
    "        else:\n",
    "            message.pretty_print()\n",
    "    return message\n",
    "\n",
    "def process_query(query, config=None):\n",
    "    inputs = {\"messages\": [(\"user\", query)]}\n",
    "    message = process_stream(graph.stream(inputs, config, stream_mode=\"values\"))\n",
    "    render_markdown(f\"## Answer:\\n{message.content}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "904f081a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f198d121",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input = \"What is the weather like in Tokyo?\"\n",
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "\n",
    "## The \"config\" is the SECODN positional argument to stream() or invoke()\n",
    "\n",
    "events = graph.stream(\n",
    "    {\"messages\": [(\"user\", user_input)]}, config, stream_mode=\"values\"\n",
    ")\n",
    "\n",
    "for event in events:\n",
    "    if \"messages\" in event:\n",
    "        event[\"messages\"][-1].pretty_print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0408723",
   "metadata": {},
   "outputs": [],
   "source": [
    "snapshot = graph.get_state(config)\n",
    "print(snapshot.next)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "337dca6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Edit Agent Actions if I don't like what they are doing...\n",
    "\n",
    "\n",
    "last_message = snatshot.values[\"messages\"][-1]\n",
    "print(\"Original Tool Call:\", last_message.tools_calls)\n",
    "print(\"Original Tool Call Message ID\", last_message.id)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be0bb9f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import AIMessage\n",
    "\n",
    "# Copy the exisintg tool call and modify the query...\n",
    "new_tool_call = last_message.tool_calls[0].copy()\n",
    "new_tool_call[\"args\"][\"query\"] = \"What was the population of Singpore in 1990, when it was just a 'backwater' country?\"\n",
    "\n",
    "\n",
    "\n",
    "## Now you have to update with new message here...\n",
    "new_message = AIMessage(\n",
    " content=last_message.content,\n",
    " tool_calls= [new_tool_call],\n",
    " id=last_message.id  ## <<-- Important to keep the same ID for replacement message\n",
    ")\n",
    "\n",
    "## print the neww tool call message...\n",
    "\n",
    "print(\"New Tool Call:\", new_tool_call)\n",
    "print(\"New Tool Call Message ID\", new_message.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d44ea628",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Update the \"State\" with the new message...\n",
    "\n",
    "graph.update_state(config, {\"messages\": [new_message]})\n",
    "print(\"Updated tool call in the graph\")\n",
    "graph.get_state(config).values[\"messages\"][-1].tool_calls\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d6536d",
   "metadata": {},
   "outputs": [],
   "source": [
    "events = graph.stream(None, config, stream_mode=\"values\")\n",
    "for event in events:\n",
    "    if \"messages\" in event:\n",
    "        event[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1230e64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da1e82d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e67b4d30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5267cc89",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (venv_new)",
   "language": "python",
   "name": "venv_new"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
